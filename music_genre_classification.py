# -*- coding: utf-8 -*-
"""Music Genre Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# music_genre_classification.py
import numpy as np
import pandas as pd
import librosa
import librosa.display
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, LSTM, BatchNormalization
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import os
import pickle
import warnings
warnings.filterwarnings('ignore')

class MusicGenreClassifier:
    def __init__(self, sample_rate=22050, duration=30):
        self.sample_rate = sample_rate
        self.duration = duration
        self.model = None
        self.label_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        self.feature_columns = []

    def extract_audio_features(self, audio_path):
        """Extract comprehensive audio features from music file"""
        try:
            # Load audio file
            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.duration)

            # Extract features
            features = {}

            # 1. Spectral Features
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            features['spectral_centroid_mean'] = np.mean(spectral_centroids)
            features['spectral_centroid_std'] = np.std(spectral_centroids)

            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)
            features['spectral_rolloff_std'] = np.std(spectral_rolloff)

            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)
            features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)

            # 2. Zero Crossing Rate
            zcr = librosa.feature.zero_crossing_rate(y)[0]
            features['zcr_mean'] = np.mean(zcr)
            features['zcr_std'] = np.std(zcr)

            # 3. MFCC (Mel-frequency cepstral coefficients)
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            for i in range(13):
                features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])
                features[f'mfcc_{i}_std'] = np.std(mfccs[i])

            # 4. Chroma Features
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            for i in range(12):
                features[f'chroma_{i}_mean'] = np.mean(chroma[i])
                features[f'chroma_{i}_std'] = np.std(chroma[i])

            # 5. Tonnetz (Tonal centroid features)
            tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)
            for i in range(6):
                features[f'tonnetz_{i}_mean'] = np.mean(tonnetz[i])
                features[f'tonnetz_{i}_std'] = np.std(tonnetz[i])

            # 6. Tempo
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            features['tempo'] = tempo

            # 7. RMS Energy
            rms = librosa.feature.rms(y=y)[0]
            features['rms_mean'] = np.mean(rms)
            features['rms_std'] = np.std(rms)

            return features

        except Exception as e:
            print(f"Error processing {audio_path}: {e}")
            return None

    def extract_spectrogram_features(self, audio_path, n_mels=128, hop_length=512):
        """Extract mel-spectrogram for CNN input"""
        try:
            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.duration)

            # Generate mel-spectrogram
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=n_mels, hop_length=hop_length
            )

            # Convert to log scale (dB)
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

            return mel_spec_db

        except Exception as e:
            print(f"Error processing spectrogram for {audio_path}: {e}")
            return None

    def load_dataset(self, dataset_path, feature_type='traditional'):
        """Load dataset and extract features"""
        print(f"Loading dataset from {dataset_path}...")

        features_list = []
        labels_list = []

        # Iterate through genre folders
        for genre in os.listdir(dataset_path):
            genre_path = os.path.join(dataset_path, genre)

            if os.path.isdir(genre_path):
                print(f"Processing {genre} genre...")

                for audio_file in os.listdir(genre_path):
                    if audio_file.endswith(('.mp3', '.wav', '.flac')):
                        audio_path = os.path.join(genre_path, audio_file)

                        if feature_type == 'traditional':
                            features = self.extract_audio_features(audio_path)
                            if features is not None:
                                features_list.append(features)
                                labels_list.append(genre)

                        elif feature_type == 'spectrogram':
                            spectrogram = self.extract_spectrogram_features(audio_path)
                            if spectrogram is not None:
                                features_list.append(spectrogram)
                                labels_list.append(genre)

        if feature_type == 'traditional':
            # Convert to DataFrame
            self.features_df = pd.DataFrame(features_list)
            self.labels = np.array(labels_list)
            self.feature_columns = list(self.features_df.columns)

            print(f"Extracted {len(self.features_df)} samples with {len(self.feature_columns)} features")
            print(f"Genres: {np.unique(self.labels)}")

        elif feature_type == 'spectrogram':
            self.spectrograms = np.array(features_list)
            self.labels = np.array(labels_list)

            print(f"Extracted {len(self.spectrograms)} spectrograms")
            print(f"Spectrogram shape: {self.spectrograms.shape}")
            print(f"Genres: {np.unique(self.labels)}")

    def preprocess_data(self, feature_type='traditional'):
        """Preprocess features and labels"""
        if feature_type == 'traditional':
            # Handle missing values
            self.features_df = self.features_df.fillna(self.features_df.mean())

            # Normalize features
            X = self.scaler.fit_transform(self.features_df)

        elif feature_type == 'spectrogram':
            # Normalize spectrograms
            X = self.spectrograms
            X = (X - X.mean()) / X.std()

            # Add channel dimension for CNN
            X = X[..., np.newaxis]

        # Encode labels
        y = self.label_encoder.fit_transform(self.labels)
        y = tf.keras.utils.to_categorical(y)

        return X, y

    def build_traditional_model(self, input_dim, num_classes):
        """Build traditional neural network for handcrafted features"""
        self.model = Sequential([
            Dense(512, activation='relu', input_shape=(input_dim,)),
            BatchNormalization(),
            Dropout(0.3),

            Dense(256, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),

            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),

            Dense(64, activation='relu'),
            Dropout(0.2),

            Dense(num_classes, activation='softmax')
        ])

        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        print("Traditional neural network model built")
        return self.model

    def build_cnn_model(self, input_shape, num_classes):
        """Build CNN model for spectrogram input"""
        self.model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(64, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(128, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(256, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Flatten(),
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(num_classes, activation='softmax')
        ])

        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        print("CNN model built")
        return self.model

    def build_rnn_model(self, input_shape, num_classes):
        """Build RNN model for sequential audio features"""
        self.model = Sequential([
            LSTM(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.3),

            LSTM(64, return_sequences=False),
            Dropout(0.3),

            Dense(64, activation='relu'),
            Dropout(0.2),

            Dense(num_classes, activation='softmax')
        ])

        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        print("RNN model built")
        return self.model

    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):
        """Train the model"""
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                patience=15, restore_best_weights=True, monitor='val_loss'
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                factor=0.2, patience=10, monitor='val_loss'
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'best_music_model.h5', save_best_only=True, monitor='val_accuracy'
            )
        ]

        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def evaluate(self, X_test, y_test):
        """Evaluate model performance"""
        # Predictions
        y_pred = self.model.predict(X_test)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_true_classes = np.argmax(y_test, axis=1)

        # Accuracy
        accuracy = accuracy_score(y_true_classes, y_pred_classes)
        print(f"Test Accuracy: {accuracy:.4f}")

        # Classification report
        genre_names = self.label_encoder.classes_
        report = classification_report(
            y_true_classes, y_pred_classes,
            target_names=genre_names
        )
        print("Classification Report:")
        print(report)

        # Confusion matrix
        cm = confusion_matrix(y_true_classes, y_pred_classes)

        # Plot confusion matrix
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=genre_names, yticklabels=genre_names)
        plt.title('Confusion Matrix - Music Genre Classification')
        plt.xlabel('Predicted Genre')
        plt.ylabel('True Genre')
        plt.tight_layout()
        plt.show()

        return accuracy, report, cm

    def plot_training_history(self, history):
        """Plot training history"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # Accuracy plot
        ax1.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
        ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')
        ax1.set_title('Model Accuracy')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True)

        # Loss plot
        ax2.plot(history.history['loss'], label='Training Loss', color='blue')
        ax2.plot(history.history['val_loss'], label='Validation Loss', color='red')
        ax2.set_title('Model Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.show()

    def predict_genre(self, audio_path, feature_type='traditional'):
        """Predict genre of a single audio file"""
        if feature_type == 'traditional':
            features = self.extract_audio_features(audio_path)
            if features is None:
                return None, 0.0

            # Convert to DataFrame and ensure same features
            features_df = pd.DataFrame([features])
            features_df = features_df.reindex(columns=self.feature_columns, fill_value=0)

            # Scale features
            X = self.scaler.transform(features_df)

        elif feature_type == 'spectrogram':
            spectrogram = self.extract_spectrogram_features(audio_path)
            if spectrogram is None:
                return None, 0.0

            # Normalize and reshape
            X = (spectrogram - spectrogram.mean()) / spectrogram.std()
            X = X[np.newaxis, ..., np.newaxis]

        # Make prediction
        prediction = self.model.predict(X, verbose=0)
        predicted_class = np.argmax(prediction)
        confidence = prediction[0][predicted_class]

        # Get genre name
        genre = self.label_encoder.inverse_transform([predicted_class])[0]

        return genre, confidence

    def analyze_audio_features(self, audio_path):
        """Analyze and visualize audio features"""
        # Load audio
        y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.duration)

        # Create visualizations
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # 1. Waveform
        axes[0, 0].plot(y)
        axes[0, 0].set_title('Waveform')
        axes[0, 0].set_xlabel('Time (samples)')
        axes[0, 0].set_ylabel('Amplitude')

        # 2. Spectrogram
        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[0, 1])
        axes[0, 1].set_title('Spectrogram')

        # 3. Mel-spectrogram
        S = librosa.feature.melspectrogram(y=y, sr=sr)
        S_db = librosa.amplitude_to_db(S, ref=np.max)
        librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[0, 2])
        axes[0, 2].set_title('Mel Spectrogram')

        # 4. MFCCs
        mfccs = librosa.feature.mfcc(y=y, sr=sr)
        librosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=axes[1, 0])
        axes[1, 0].set_title('MFCCs')

        # 5. Chroma
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        librosa.display.specshow(chroma, sr=sr, x_axis='time', y_axis='chroma', ax=axes[1, 1])
        axes[1, 1].set_title('Chroma Features')

        # 6. Spectral centroid
        cent = librosa.feature.spectral_centroid(y=y, sr=sr)
        times = librosa.frames_to_time(np.arange(len(cent[0])), sr=sr)
        axes[1, 2].plot(times, cent[0])
        axes[1, 2].set_title('Spectral Centroid')
        axes[1, 2].set_xlabel('Time (s)')

        plt.tight_layout()
        plt.show()

        # Extract and display numerical features
        features = self.extract_audio_features(audio_path)
        if features:
            print("Audio Features:")
            for key, value in features.items():
                print(f"{key}: {value:.4f}")

    def save_model(self, model_path='music_genre_model.h5'):
        """Save the trained model and preprocessors"""
        self.model.save(model_path)

        # Save preprocessors
        with open('music_preprocessors.pkl', 'wb') as f:
            pickle.dump({
                'label_encoder': self.label_encoder,
                'scaler': self.scaler,
                'feature_columns': self.feature_columns
            }, f)

        print(f"Model saved to {model_path}")
        print("Preprocessors saved to music_preprocessors.pkl")

    def load_model(self, model_path='music_genre_model.h5'):
        """Load trained model and preprocessors"""
        self.model = tf.keras.models.load_model(model_path)

        # Load preprocessors
        with open('music_preprocessors.pkl', 'rb') as f:
            data = pickle.load(f)
            self.label_encoder = data['label_encoder']
            self.scaler = data['scaler']
            self.feature_columns = data['feature_columns']

        print("Model and preprocessors loaded successfully")

def demo_music_analysis(classifier, audio_path):
    """Demonstrate music analysis capabilities"""
    print(f"Analyzing: {audio_path}")
    print("-" * 50)

    # Predict genre
    genre, confidence = classifier.predict_genre(audio_path)
    print(f"Predicted Genre: {genre}")
    print(f"Confidence: {confidence:.4f}")
    print("-" * 50)

    # Analyze features
    classifier.analyze_audio_features(audio_path)

def main():
    """Main function demonstrating the music genre classification system"""
    # Initialize classifier
    classifier = MusicGenreClassifier(sample_rate=22050, duration=30)

    # Dataset path (adjust as needed)
    dataset_path = 'music_dataset'  # Should contain genre folders with audio files

    if os.path.exists(dataset_path):
        # Load dataset with traditional features
        print("Training with traditional audio features...")
        classifier.load_dataset(dataset_path, feature_type='traditional')

        # Preprocess data
        X, y = classifier.preprocess_data(feature_type='traditional')

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )

        # Build and train model
        num_features = X_train.shape[1]
        num_classes = y_train.shape[1]

        classifier.build_traditional_model(num_features, num_classes)
        history = classifier.train(X_train, y_train, X_val, y_val, epochs=100)

        # Plot training history
        classifier.plot_training_history(history)

        # Evaluate model
        classifier.evaluate(X_test, y_test)

        # Save model
        classifier.save_model()

        # Demo prediction (replace with actual audio file)
        # demo_audio = 'path/to/test/audio.wav'
        # if os.path.exists(demo_audio):
        #     demo_music_analysis(classifier, demo_audio)

    else:
        print(f"Dataset path {dataset_path} not found!")
        print("Please organize your dataset as:")
        print("music_dataset/")
        print("  blues/")
        print("    song1.wav")
        print("    song2.wav")
        print("  rock/")
        print("    song1.wav")
        print("    song2.wav")
        print("  ...")

if __name__ == "__main__":
    main()